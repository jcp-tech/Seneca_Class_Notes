{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7a2a14",
   "metadata": {},
   "source": [
    "# [AIG100 – Project 3: Income Classification using Neural Networks](hhttp://github.com/jcp-tech/Seneca_Class_Notes/blob/master/Semester%201/AIG100%20-%20Machine%20Learing/Project%203/Project%203%20-%20AIG100%20-%20Jonathan%20Chacko.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11a9dd",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Dataset Overview & Significance](#Dataset-Overview--Significance)\n",
    "3. [Data Cleaning & Preprocessing](#Data-Cleaning--Preprocessing)\n",
    "   - [Loading the Dataset](#Loading-the-Dataset)\n",
    "   - [Data Cleaning Steps](#Data-Cleaning-Steps)\n",
    "4. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "   - [Analysis of Numerical Features](#Analysis-of-Numerical-Features)\n",
    "   - [Analysis of Categorical Features](#Analysis-of-Categorical-Features)\n",
    "5. [Data Preprocessing & Feature Engineering](#Data-Preprocessing--Feature-Engineering)\n",
    "   - [Train-Test Split and Class Distribution](#Train-Test-Split-and-Class-Distribution)\n",
    "   - [Handling Class Imbalance](#Handling-Class-Imbalance)\n",
    "6. [Modeling Approach](#Modeling-Approach)\n",
    "7. [Evaluation & Results](#Evaluation--Results)\n",
    "   - [Cross-Validation Performance](#Cross-Validation-Performance)\n",
    "   - [Model Architecture Summary](#Model-Architecture-Summary)\n",
    "   - [Learning Curves](#Learning-Curves)\n",
    "   - [Final Test Set Evaluation](#Final-Test-Set-Evaluation)\n",
    "   - [Confusion Matrix Visualization](#Confusion-Matrix-Visualization)\n",
    "   - [Per-Class Performance Metrics](#Per-Class-Performance-Metrics)\n",
    "   - [ROC Curve Analysis](#ROC-Curve-Analysis)\n",
    "   - [Precision-Recall Curve](#Precision-Recall-Curve)\n",
    "8. [Model Interpretation & Insights](#Model-Interpretation--Insights)\n",
    "   - [Key Findings](#Key-Findings)\n",
    "   - [Model Performance Analysis](#Model-Performance-Analysis)\n",
    "   - [Feature Importance Observations](#Feature-Importance-Observations)\n",
    "9. [Real-World Applications & Implications](#Real-World-Applications--Implications)\n",
    "   - [Use Cases in Financial Services](#Use-Cases-in-Financial-Services)\n",
    "   - [Ethical Considerations](#Ethical-Considerations)\n",
    "   - [Limitations & Potential Improvements](#Limitations--Potential-Improvements)\n",
    "10. [Reflection](#Reflection)\n",
    "11. [Conclusion](#Conclusion)\n",
    "12. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ad8cc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Project Title\n",
    "**Predicting Loan Default Risk Using Census Income Data**\n",
    "\n",
    "### Objective\n",
    "To build a neural network model that classifies individuals as low-risk or high-risk loan applicants, based on demographic and socio-economic data from the UCI Census Income dataset.\n",
    "\n",
    "### Background\n",
    "Financial institutions face the constant challenge of determining whether a loan applicant is likely to default. Traditionally, this involves credit history and financial statements—but what if we could leverage broader demographic and employment features to predict credit risk?\n",
    "\n",
    "By classifying whether a person earns more than $50K per year using neural networks, we indirectly estimate their income capacity, a crucial factor in loan repayment ability. High-income individuals are typically lower-risk, while lower-income applicants may pose greater risk, especially without collateral or prior credit history.\n",
    "\n",
    "<!-- \n",
    "### Note\n",
    "I decided to Approach this Project with a more practical Perspective to see if i can solve a Problem with the Data in Hand. \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9d32c",
   "metadata": {},
   "source": [
    "## Dataset Overview & Significance\n",
    "\n",
    "### Dataset Details\n",
    "- **Source**: [UCI Census Income Dataset](https://archive.ics.uci.edu/dataset/20/census+income)\n",
    "- **Structure**: ~48,842 rows, 14 feature columns plus target variable\n",
    "- **Target Variable**: `income` (binary: >$50K or ≤$50K)\n",
    "<!-- - **Class Distribution**: Approximately 75% ≤$50K, 25% >$50K (imbalanced) -->\n",
    "\n",
    "### Why This Project Matters\n",
    "\n",
    "This project addresses a critical business need in the financial sector: accurately assessing loan default risk. Traditional methods rely heavily on credit history, which disadvantages those with limited credit records (the \"thin file\" problem). By leveraging demographic and socioeconomic data, we can create more inclusive risk assessment models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bdc59",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea6cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General Libraries ===\n",
    "import warnings, os, math, shutil, json\n",
    "# from IPython.display import Markdown, display\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder # , OrdinalEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === Evaluation Metrics ===\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, # f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    # mean_squared_error, r2_score, mean_absolute_error,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    roc_curve, roc_auc_score # , auc\n",
    ")\n",
    "\n",
    "# === TensorFlow / Keras ===\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, BatchNormalization,\n",
    "    # Input, Flatten, Embedding, Concatenate, Reshape,\n",
    "    LeakyReLU\n",
    ")\n",
    "# from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5db3cf",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "census_income = fetch_ucirepo(id=20) # https://archive.ics.uci.edu/dataset/20/census+income\n",
    "# print(census_income.metadata) # metadata \n",
    "# print(census_income.variables) # variable information \n",
    "df = pd.concat([census_income.data.features, census_income.data.targets], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c408c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74226df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the Values in Each Column\n",
    "for col in df.columns:\n",
    "    print(f\"Column: `{col}`\")\n",
    "    print(df[col].unique())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94788491",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=['int64', 'float64']) # Create a copy of the dataframe with only numeric columns\n",
    "corr_matrix = numeric_df.corr() # Compute correlation matrix\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # Apply mask\n",
    "corr_matrix_trimmed = corr_matrix.iloc[1:, :-1]\n",
    "mask_trimmed = mask[1:, :-1]\n",
    "plt.figure(figsize=(15, 10))# Plot heatmap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(\n",
    "    corr_matrix_trimmed,\n",
    "    mask=mask_trimmed,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.5}\n",
    ")\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf52a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44734d97",
   "metadata": {},
   "source": [
    "### Data Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ccb7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handling missing values (replacing `'?'` with `NaN` and dropping rows)\n",
    "df[df=='?']=np.nan # Replacing '?' with NaN\n",
    "df=df.dropna(axis=0) # Dropping all rows with NaN values\n",
    "# df = df[~(df['native-country'] == \"?\") | (df['native-country'].isnull())] # ALTERNATIVE Removing all rows with Unknown Countries as part of data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transforming categorical variables (age binning)\n",
    "df['age'] = pd.cut(df['age'], bins=[0, 30, 60, 100], labels=['Young', 'Middle-aged', 'Old']) # .astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature engineering (creating capital-profit from gain/loss)\n",
    "df['capital-metrics'] = df['capital-gain'] - df['capital-loss']\n",
    "df['capital-profit'] = df['capital-metrics'].apply(lambda x: 0 if x < 0 else 1)\n",
    "df.drop(columns=['capital-gain', 'capital-loss', 'capital-metrics'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # check the data types and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d94060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Removing irrelevant features\n",
    "df.drop(columns=['fnlwgt', 'education'], inplace=True) # Dropping the fnlwgt and education columns as they are not relevant to the analysis\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Standardizing the target variable\n",
    "df['income'] = df['income'].str.replace('.', '')\n",
    "df['income'].replace({'<=50K': 0, '>50K': 1}, inplace=True)\n",
    "# df['income'] = df['income'].astype('category')\n",
    "df['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16686440",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features\n",
    "numerical_features_temp = df.select_dtypes(include=['int64', 'float64']).drop(columns=['income', 'capital-profit']).columns.tolist()\n",
    "categorical_features_temp = df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325e912",
   "metadata": {},
   "source": [
    "### Analysis of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9579eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NUMERICAL FEATURES ===\n",
    "num_cols = 2\n",
    "num_rows = math.ceil(len(numerical_features_temp) / num_cols)\n",
    "plt.figure(figsize=(num_cols * 6, num_rows * 4))\n",
    "\n",
    "for i, feature in enumerate(numerical_features_temp):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    sns.boxplot(data=df, x='income', y=feature)\n",
    "    plt.title(f\"{feature} by Income\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle(\"Numerical Features Distribution by Income\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2db98b",
   "metadata": {},
   "source": [
    "### Analysis of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CATEGORICAL FEATURES ===\n",
    "cat_cols = 2\n",
    "cat_rows = math.ceil(len(categorical_features_temp) / cat_cols)\n",
    "plt.figure(figsize=(cat_cols * 6, cat_rows * 4))\n",
    "\n",
    "for i, feature in enumerate(categorical_features_temp):\n",
    "    plt.subplot(cat_rows, cat_cols, i + 1)\n",
    "\n",
    "    # Get top 10 most common categories (if needed)\n",
    "    top_categories = df[feature].value_counts().nlargest(10).index\n",
    "    filtered_df = df[df[feature].isin(top_categories)]\n",
    "\n",
    "    sns.countplot(data=filtered_df, x=feature, hue='income')\n",
    "    plt.title(f\"{feature} vs Income\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle(\"Categorical Features Comparison by Income.\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3b1ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa72fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'income' # Setting the target variable\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "numeric_features = [f for f in df.select_dtypes(include=[np.number, 'number', 'float64', 'int64', 'float32', 'int32']).columns.to_list() if f != target]\n",
    "categorical_features = [f for f in df.select_dtypes(include=[\"object\", \"category\"]).columns.to_list() if f != target ]\n",
    "\n",
    "# Numeric pipeline with standard scaling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "# Categorical pipeline with one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(\n",
    "        # drop='first',\n",
    "        # sparse_output=False, # Return dense array\n",
    "        handle_unknown=\"ignore\"\n",
    "    )),\n",
    "    # ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2a584",
   "metadata": {},
   "source": [
    "### Train-Test Split and Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010921b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ca2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: separate out the test set (80% train+val, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the split proportions\n",
    "print(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/X.shape[0]:.2%} of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab9195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in each set\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Overall: {y.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in the dataset\n",
    "class_distribution = y.value_counts(normalize=True)\n",
    "plt.figure(figsize=(15, 10)) # Create a bar plot of the class distribution\n",
    "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
    "plt.title('Class Distribution in Dataset')\n",
    "plt.xlabel('Income Group (0: ≤$50K, 1: >$50K)')\n",
    "plt.ylabel('Proportion')\n",
    "for i, p in enumerate(ax.patches): # Add percentage labels on top of the bars\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height:.1%}', (p.get_x() + p.get_width() / 2, height),\n",
    "                ha='center', va='bottom')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "print(r\"Class distribution in dataset:\")\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_dataset_class_distribution(class_distribution):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Class Distribution in Full Dataset\")\n",
    "    # Ensure it's a dictionary\n",
    "    if hasattr(class_distribution, 'to_dict'):\n",
    "        class_distribution = class_distribution.to_dict()\n",
    "    for cls, proportion in class_distribution.items():\n",
    "        interpretation.append(f\"- Class {cls}: {proportion:.1%} of total samples\")\n",
    "    majority_class = max(class_distribution, key=class_distribution.get)\n",
    "    minority_class = min(class_distribution, key=class_distribution.get)\n",
    "    majority_ratio = class_distribution[majority_class]\n",
    "    minority_ratio = class_distribution[minority_class]\n",
    "    imbalance_ratio = majority_ratio / minority_ratio\n",
    "    interpretation.append(f\"\\n- Imbalance ratio (majority:minority) => {imbalance_ratio:.2f}:1\")\n",
    "    if imbalance_ratio > 1.5:\n",
    "        interpretation.append(\"\\nThere is a significant class imbalance in the dataset.\")\n",
    "        interpretation.append(\"Without corrective measures, the model may favor the majority class.\")\n",
    "        interpretation.append(\"To address this, techniques like class weighting, resampling, or threshold tuning are recommended.\")\n",
    "    else:\n",
    "        interpretation.append(\"\\nThe dataset is relatively balanced, so class imbalance should not heavily impact performance.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_dataset_class_distribution(class_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e379d",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our dataset has an imbalance we'll calculate class weights to ensure the model pays sufficient attention to the minority class during training.\n",
    "y_train_series = pd.Series(y_train)\n",
    "class_counts = y_train_series.value_counts().sort_index()\n",
    "classes = np.array(class_counts.index)  # instead of .to_list()\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_series)\n",
    "class_weight_dict = dict(zip(classes, weights))\n",
    "# class_weight_dict = {i: weight for i, weight in enumerate(len(y_train) / (len(np.unique(y_train)) * np.bincount(y_train)))}\n",
    "print(r\"Class weights for model training:\")\n",
    "print(json.dumps({int(key):float(value) for key, value in class_weight_dict.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25fd2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Plot 1: Class Distribution\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, ax=ax[0])\n",
    "ax[0].set_title('Class Distribution in y_train')\n",
    "ax[0].set_xlabel('Class Label')\n",
    "ax[0].set_ylabel('Number of Samples')\n",
    "# Plot 2: Computed Class Weights\n",
    "sns.barplot(x=list(class_weight_dict.keys()), y=list(class_weight_dict.values()), ax=ax[1], palette=\"mako\")\n",
    "ax[1].set_title('Computed Class Weights')\n",
    "ax[1].set_xlabel('Class Label')\n",
    "ax[1].set_ylabel('Weight Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_class_weights(class_counts, class_weight_dict):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Class Distribution and Computed Weights Interpretation\")\n",
    "    # Convert to dictionary if it's a pandas Series\n",
    "    if hasattr(class_counts, 'to_dict'):\n",
    "        class_counts = class_counts.to_dict()\n",
    "    total_samples = sum(class_counts.values())\n",
    "    majority_class = max(class_counts, key=class_counts.get)\n",
    "    minority_class = min(class_counts, key=class_counts.get)\n",
    "    majority_count = class_counts[majority_class]\n",
    "    minority_count = class_counts[minority_class]\n",
    "    interpretation.append(f\"- Total training samples: {total_samples}\")\n",
    "    interpretation.append(f\"- Class {majority_class} has {majority_count} samples.\")\n",
    "    interpretation.append(f\"- Class {minority_class} has {minority_count} samples.\")\n",
    "    imbalance_ratio = majority_count / minority_count\n",
    "    interpretation.append(f\"- Class imbalance ratio (majority/minority): {imbalance_ratio:.2f}\")\n",
    "    interpretation.append(\"\\nComputed Class Weights:\")\n",
    "    for cls, weight in class_weight_dict.items():\n",
    "        interpretation.append(f\"- Class {cls}: Weight = {weight:.2f}\")\n",
    "    if imbalance_ratio > 1.5:\n",
    "        interpretation.append(\"\\nThe dataset is imbalanced. The Variable `class_weight_dict` is used to address this by giving more importance to the minority class during training.\")\n",
    "        interpretation.append(\"This encourages the model to pay equal attention to both classes by increasing the penalty for misclassifying underrepresented samples.\")\n",
    "        interpretation.append(\"As a result, class_weight helps the model avoid biasing predictions toward the majority class.\")\n",
    "    else:\n",
    "        interpretation.append(\"\\nThe class distribution is relatively balanced, so class weights may have limited effect.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_class_weights(class_counts, class_weight_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b2e01",
   "metadata": {},
   "source": [
    "## Modeling Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d672b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics and models\n",
    "fold_metrics = []\n",
    "fold_models = []\n",
    "\n",
    "best_metric = -np.inf  # Initialize to very low number\n",
    "best_fold_index = -1\n",
    "# best_model = None\n",
    "\n",
    "folder_path = \"ignore/fold_models\" # os.makedirs(\"ignore/fold_models\", exist_ok=True)\n",
    "\n",
    "# If the folder exists, empty it\n",
    "if os.path.exists(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # remove file or link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # remove subdirectory\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "else:\n",
    "    os.makedirs(folder_path)  # create the folder if it doesn't exist\n",
    "\n",
    "\n",
    "fold_historys = {}\n",
    "\n",
    "# Configuration based on seeking high robustness and accuracy, ignoring time constraints:\n",
    "number_of_folds = 10  # Use 10 folds for a robust cross-validation estimate of performance.\n",
    "number_of_epochs = 300 # Set epochs high; this will be controlled by early stopping.\n",
    "\n",
    "# KFold CV\n",
    "kf = KFold(n_splits=number_of_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f'\\nFold {fold + 1}/{number_of_folds}')\n",
    "    checkpoint_path = f'{folder_path}/fold_{fold + 1}.keras'\n",
    "\n",
    "    # Train/Val Split\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Preprocessing\n",
    "    X_fold_train_processed = preprocessor.fit_transform(X_fold_train).astype('float32')\n",
    "    X_fold_val_processed = preprocessor.transform(X_fold_val).astype('float32')\n",
    "    # X_test_processed = preprocessor.transform(X_test).astype('float32')\n",
    "\n",
    "    # Model\n",
    "    input_dim = X_fold_train_processed.shape[1]\n",
    "    keras_model = Sequential([\n",
    "        # Input-level dropout to add noise and prevent overfitting from the start\n",
    "        Dropout(0.1, input_shape=(input_dim,)),\n",
    "\n",
    "        # First Dense layer with He initialization (good for ReLU/LeakyReLU)\n",
    "        # Followed by LeakyReLU to avoid dead neuron problems\n",
    "        Dense(256, kernel_initializer='he_normal'),\n",
    "        LeakyReLU(alpha=0.1),  # Allows small gradients when input < 0\n",
    "        BatchNormalization(),  # Normalize activations to improve convergence\n",
    "        Dropout(0.3),          # Dropout to reduce overfitting\n",
    "\n",
    "        # Second Dense layer with ReLU activation\n",
    "        # BatchNorm and Dropout help regularize this layer\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Third Dense layer, similar structure\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Output layer for binary classification (sigmoid outputs probability)\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    keras_model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=15,\n",
    "        min_delta=0.001,  # minimum improvement threshold\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_auc',  # track AUC for model selection\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    history = keras_model.fit(\n",
    "        X_fold_train_processed, y_fold_train,\n",
    "        validation_data=(X_fold_val_processed, y_fold_val),\n",
    "        epochs=number_of_epochs,\n",
    "        batch_size=64,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping, model_checkpoint], # Save the best model based on AUC\n",
    "        verbose=0,\n",
    "        shuffle=True # Shuffle the training data for each epoch\n",
    "    )\n",
    "\n",
    "    fold_historys[fold + 1] = history.history\n",
    "\n",
    "    # Evaluation\n",
    "    _, accuracy, precision, recall, auc = keras_model.evaluate(X_fold_val_processed, y_fold_val, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc\n",
    "    })\n",
    "    \n",
    "    fold_models.append(keras_model)\n",
    "\n",
    "    print(f'Fold {fold + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, AUC: {auc:.4f}')\n",
    "\n",
    "    # Track best model by AUC\n",
    "    if auc > best_metric:\n",
    "        best_metric = auc\n",
    "        # best_model = keras_model\n",
    "        best_fold_index = fold + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(precision, recall):\n",
    "    return [2 * (p * r) / (p + r) if (p + r) > 0 else 0 for p, r in zip(precision, recall)]\n",
    "\n",
    "for fold_num, history in fold_historys.items():\n",
    "    print(f\"Fold {fold_num} Training History\")\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(4, 2)\n",
    "\n",
    "    # Subplots for rows 1 to 3\n",
    "    ax_accuracy = fig.add_subplot(gs[0, 0])\n",
    "    ax_loss = fig.add_subplot(gs[0, 1])\n",
    "    ax_precision = fig.add_subplot(gs[1, 0])\n",
    "    ax_recall = fig.add_subplot(gs[1, 1])\n",
    "    ax_auc = fig.add_subplot(gs[2, 0])\n",
    "    ax_f1 = fig.add_subplot(gs[2, 1])\n",
    "    # ax_gap = fig.add_subplot(gs[3, 0])      # ← Row 4 (AUC Gap) - Commented\n",
    "    # ax_lr = fig.add_subplot(gs[3, 1])       # ← Row 4 (Learning Rate) - Commented\n",
    "\n",
    "    # Standard metric plots\n",
    "    base_metrics = [\n",
    "        ('accuracy', ax_accuracy),\n",
    "        ('loss', ax_loss),\n",
    "        ('precision', ax_precision),\n",
    "        ('recall', ax_recall),\n",
    "        ('auc', ax_auc),\n",
    "    ]\n",
    "\n",
    "    for metric, ax in base_metrics:\n",
    "        ax.plot(history[metric], label='Train', linewidth=2)\n",
    "        ax.plot(history[f'val_{metric}'], label='Validation', linestyle='--', linewidth=2)\n",
    "        ax.set_title(f'{metric.title()}')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric.title())\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # F1 Score plot\n",
    "    train_f1 = compute_f1(history['precision'], history['recall'])\n",
    "    val_f1 = compute_f1(history['val_precision'], history['val_recall'])\n",
    "    ax_f1.plot(train_f1, label='Train F1', linewidth=2)\n",
    "    ax_f1.plot(val_f1, label='Val F1', linestyle='--', linewidth=2)\n",
    "    ax_f1.set_title('F1 Score')\n",
    "    ax_f1.set_xlabel('Epoch')\n",
    "    ax_f1.set_ylabel('F1 Score')\n",
    "    ax_f1.legend()\n",
    "    ax_f1.grid(True)\n",
    "\n",
    "    # AUC Gap Plot\n",
    "    # auc_gap = [tr - val for tr, val in zip(history['auc'], history['val_auc'])]\n",
    "    # ax_gap.plot(auc_gap, color='orange', label='AUC Gap (Train - Val)', linewidth=2)\n",
    "    # ax_gap.axhline(0, color='gray', linestyle='--')\n",
    "    # ax_gap.set_title('AUC Gap')\n",
    "    # ax_gap.set_xlabel('Epoch')\n",
    "    # ax_gap.set_ylabel('Gap')\n",
    "    # ax_gap.legend()\n",
    "    # ax_gap.grid(True)\n",
    "\n",
    "    # Learning Rate Plot\n",
    "    # if 'lr' in history:\n",
    "    #     ax_lr.plot(history['lr'], label='Learning Rate', color='purple', linewidth=2)\n",
    "    #     ax_lr.set_title('Learning Rate')\n",
    "    #     ax_lr.set_xlabel('Epoch')\n",
    "    #     ax_lr.set_ylabel('LR')\n",
    "    #     ax_lr.legend()\n",
    "    #     ax_lr.grid(True)\n",
    "    # else:\n",
    "    #     ax_lr.text(0.5, 0.5, 'No LR Data', ha='center', va='center', fontsize=12)\n",
    "    #     ax_lr.set_title('Learning Rate')\n",
    "    #     ax_lr.axis('off')\n",
    "\n",
    "    fig.suptitle(f'Fold {fold_num} - Training Metrics', fontsize=18, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average metrics\n",
    "avg_metrics = {\n",
    "    metric: np.mean([fold[metric] for fold in fold_metrics])\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'auc']\n",
    "}\n",
    "\n",
    "print('\\nAverage metrics across all folds:')\n",
    "print(f\"Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {avg_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {avg_metrics['recall']:.4f}\")\n",
    "print(f\"AUC: {avg_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a574e7",
   "metadata": {},
   "source": [
    "## Evaluation & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c8038",
   "metadata": {},
   "source": [
    "### Cross-Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(fold_metrics) # Convert to DataFrame\n",
    "plt.figure(figsize=(15, 10)) # Plotting\n",
    "# df_metrics.plot(x='fold', y=['accuracy', 'precision', 'recall', 'auc'], kind='bar', figsize=(10, 5))\n",
    "for metric in ['accuracy', 'precision', 'recall', 'auc']:\n",
    "    plt.plot(df_metrics['fold'], df_metrics[metric], marker='o', label=metric)\n",
    "plt.title('K-Fold Cross-Validation Metrics') # plt.title('Fold-wise Evaluation Metrics')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Score')\n",
    "# plt.xticks(rotation=0)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.grid(True) # plt.grid(axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd024fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_kfold_metrics(df_metrics):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"K-Fold Cross-Validation Metrics Summary\")\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'auc']\n",
    "    for metric in metrics:\n",
    "        mean = df_metrics[metric].mean()\n",
    "        std = df_metrics[metric].std()\n",
    "        min_val = df_metrics[metric].min()\n",
    "        max_val = df_metrics[metric].max()\n",
    "        interpretation.append(f\"\\n{metric.capitalize()} Summary:\")\n",
    "        interpretation.append(f\"- Mean: {mean:.4f}\")\n",
    "        interpretation.append(f\"- Standard Deviation: {std:.4f}\")\n",
    "        interpretation.append(f\"- Min: {min_val:.4f}, Max: {max_val:.4f}\")\n",
    "        if std < 0.015:\n",
    "            interpretation.append(\"  The performance is highly consistent across folds.\")\n",
    "        elif std < 0.03:\n",
    "            interpretation.append(\"  The model is reasonably stable across folds.\")\n",
    "        else:\n",
    "            interpretation.append(\"  There is noticeable variation; fold-specific results should be reviewed.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_kfold_metrics(df_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report best fold\n",
    "print(f\"\\nBest Model is from Fold {best_fold_index} with AUC: {best_metric:.4f}\")\n",
    "best_model_path = f'{folder_path}/fold_{best_fold_index}.keras'\n",
    "final_model = load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24763b5c",
   "metadata": {},
   "source": [
    "### Model Architecture Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    final_model,\n",
    "    to_file='ignore/model_plot.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed62f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4fcbfe",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_history = fold_historys[best_fold_index - 1]\n",
    "best_epoch_index = np.argmax(best_history['val_auc']) # Get metrics from best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "# # Accuracy curves\n",
    "# plt.plot(best_history['accuracy'], label='Train Accuracy', color='blue')\n",
    "# plt.plot(best_history['val_accuracy'], label='Validation Accuracy', color='blue', linestyle='--')\n",
    "# # Loss curves (scaled to match accuracy axis)\n",
    "# plt.plot(best_history['loss'], label='Train Loss', color='green')\n",
    "# plt.plot(best_history['val_loss'], label='Validation Loss', color='green', linestyle='--')\n",
    "# # Best epoch vertical line\n",
    "# plt.axvline(best_epoch_index, color='red', linestyle='--', label=f'Best Epoch ({best_epoch_index})')\n",
    "epochs = np.arange(len(best_history['accuracy']))\n",
    "plt.subplot(1, 2, 1) # Plot training accuracy values\n",
    "plt.plot(best_history['accuracy'], label='Train')\n",
    "plt.plot(best_history['val_accuracy'], label='Validation')\n",
    "plt.axvline(best_epoch_index + 1, color='red', linestyle='--', label=f'Best Epoch ({best_epoch_index + 1})')\n",
    "plt.xticks(epochs)  # Set x-axis ticks to every epoch\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.subplot(1, 2, 2) # Plot training loss values\n",
    "plt.plot(best_history['loss'], label='Train')\n",
    "plt.plot(best_history['val_loss'], label='Validation')\n",
    "plt.axvline(best_epoch_index + 1, color='red', linestyle='--', label=f'Best Epoch ({best_epoch_index + 1})')\n",
    "plt.xticks(epochs)  # Set x-axis ticks to every epoch\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# plt.title('Model Accuracy and Loss Over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Metric Value')\n",
    "# plt.legend()\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print information about training\n",
    "print(f\"Training completed after {len(best_history['loss'])} epochs\")\n",
    "print(f\"Best epoch based on val AUC: {best_epoch_index + 1}\")\n",
    "print(f\"Final training loss: {best_history['loss'][-1]:.4f}\")\n",
    "print(f\"Final training accuracy: {best_history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {best_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {best_history['val_accuracy'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpetation\n",
    "def interpret_learning_curves(history, best_epoch_index):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Training Curve Interpretation\")\n",
    "    num_epochs = len(history['loss'])\n",
    "    final_train_acc = history['accuracy'][-1]\n",
    "    final_val_acc = history['val_accuracy'][-1]\n",
    "    final_train_loss = history['loss'][-1]\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    interpretation.append(f\"- Training completed over {num_epochs} epochs.\")\n",
    "    interpretation.append(f\"- Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "    interpretation.append(f\"- Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    interpretation.append(f\"- Final Training Loss: {final_train_loss:.4f}\")\n",
    "    interpretation.append(f\"- Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    # Overfitting check\n",
    "    if final_val_acc > final_train_acc and final_val_loss < final_train_loss:\n",
    "        interpretation.append(\"\\nThe validation accuracy is slightly higher than training accuracy, and validation loss is lower. This suggests that the model generalizes well.\")\n",
    "    elif final_val_acc < final_train_acc and final_val_loss > final_train_loss:\n",
    "        interpretation.append(\"\\nThere may be signs of overfitting, as validation accuracy is lower and validation loss is higher than training performance.\")\n",
    "    else:\n",
    "        interpretation.append(\"\\nTraining and validation curves appear fairly close, suggesting reasonable generalization.\")\n",
    "    interpretation.append(f\"\\nNOTE= Best epoch based on validation AUC: {best_epoch_index + 1}\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_learning_curves(best_history, best_epoch_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6819bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation metrics at best epoch\n",
    "val_accuracy = best_history['val_accuracy'][best_epoch_index]\n",
    "val_precision = best_history['val_precision'][best_epoch_index]\n",
    "val_recall = best_history['val_recall'][best_epoch_index]\n",
    "val_auc = best_history['val_auc'][best_epoch_index]\n",
    "val_loss = best_history['val_loss'][best_epoch_index]\n",
    "# val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall)\n",
    "\n",
    "print(\"Best Epoch Number is:\", best_epoch_index + 1, \"\\nfrom Fold Number\", best_fold_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe58f0",
   "metadata": {},
   "source": [
    "### Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test set\n",
    "X_test_processed = preprocessor.transform(X_test).astype('float32')\n",
    "# Predict and evaluate\n",
    "y_test_pred_probs = final_model.predict(X_test_processed).flatten()\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(\"int32\") # Convert probabilities to binary predictions\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5daac1",
   "metadata": {},
   "source": [
    "### Get test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = final_model.evaluate(X_test_processed, y_test, verbose=0)[0]\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) # final_model.evaluate(X_test_processed, y_test, verbose=0)[1]\n",
    "test_precision = precision_score(y_test, y_test_pred) # final_model.evaluate(X_test_processed, y_test, verbose=0)[2]\n",
    "test_recall = recall_score(y_test, y_test_pred) # final_model.evaluate(X_test_processed, y_test, verbose=0)[3]\n",
    "test_auc = roc_auc_score(y_test, y_test_pred_probs) # final_model.evaluate(X_test_processed, y_test, verbose=0)[4]\n",
    "# test_f1 = f1_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e27522",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cr = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "cr_df = pd.DataFrame(cr).transpose()\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred_probs)\n",
    "average_precision = average_precision_score(y_test, y_test_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57664baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Accuracy\": {\n",
    "        \"VALIDATION\": val_accuracy,\n",
    "        \"TEST\": test_accuracy\n",
    "    },\n",
    "    \"Precision\": {\n",
    "        \"VALIDATION\": val_precision,\n",
    "        \"TEST\": test_precision\n",
    "    },\n",
    "    \"Recall\": {\n",
    "        \"VALIDATION\": val_recall,\n",
    "        \"TEST\": test_recall\n",
    "    },\n",
    "    \"AUC\": {\n",
    "        \"VALIDATION\": val_auc,\n",
    "        \"TEST\": test_auc\n",
    "    },\n",
    "    \"Loss\": {\n",
    "        \"VALIDATION\": val_loss,\n",
    "        \"TEST\": test_loss\n",
    "    }\n",
    "    # \"F1\": {\n",
    "    #     \"VALIDATION\": val_f1,\n",
    "    #     \"TEST\": test_f1\n",
    "    # }\n",
    "}\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f5d8c",
   "metadata": {},
   "source": [
    "### Plotting the metrics to Check for Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d708ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values for plotting\n",
    "metric_names = list(metrics.keys())\n",
    "val_scores = [metrics[m][\"VALIDATION\"] for m in metric_names]\n",
    "test_scores = [metrics[m][\"TEST\"] for m in metric_names]\n",
    "\n",
    "# Positioning and bar plot setup\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Plotting the bars\n",
    "bars1 = plt.bar(x - width/2, val_scores, width, label='Validation (Best Epoch)', color='skyblue')\n",
    "bars2 = plt.bar(x + width/2, test_scores, width, label='Test Set', color='salmon')\n",
    "# Plot customization\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation (Best Epoch) vs Test Performance')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1 if max(max(val_scores), max(test_scores)) <= 1 else max(max(val_scores), max(test_scores)) + 0.1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "# Annotate bars with values\n",
    "for i in range(len(metrics)):\n",
    "    plt.text(x[i] - width/2, val_scores[i] + 0.01, f\"{val_scores[i]:.2f}\", ha='center', va='bottom')\n",
    "    plt.text(x[i] + width/2, test_scores[i] + 0.01, f\"{test_scores[i]:.2f}\", ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def generate_interpretation(metrics):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Validation vs Test Performance Interpretation =\")\n",
    "    for metric, scores in metrics.items():\n",
    "        val = scores[\"VALIDATION\"]\n",
    "        test = scores[\"TEST\"]\n",
    "        diff = val - test\n",
    "        direction = (\n",
    "            \"higher\" if val > test else\n",
    "            \"lower\" if val < test else\n",
    "            \"equal\"\n",
    "        )\n",
    "        interpretation.append(\n",
    "            f\"- {metric}: Validation score is {val:.2f}, Test score is {test:.2f} — Test is {direction} by {abs(diff):.2f}.\"\n",
    "        )\n",
    "    # Simple overfitting logic: if more than 2 metrics have validation > test by 0.02\n",
    "    overfitting_flags = sum(\n",
    "        1 for m in metrics if metrics[m][\"VALIDATION\"] - metrics[m][\"TEST\"] > 0.02\n",
    "    )\n",
    "    if overfitting_flags >= 2:\n",
    "        interpretation.append(\n",
    "            \"\\nHENCE:\\tPotential Overfitting Detected - \"\n",
    "            \"Several validation scores are notably higher than test scores, \"\n",
    "            \"suggesting the model may have learned patterns specific to the validation set \"\n",
    "            \"that do not generalize well.\"\n",
    "        )\n",
    "    else:\n",
    "        interpretation.append(\n",
    "            \"\\nHENCE:\\tNo Major Overfitting - The performance across validation and test sets \"\n",
    "            \"is consistent, indicating good generalization.\"\n",
    "        )\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(generate_interpretation(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf6036",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))  # Adjust size as needed\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=50K', '>50K']) # Confusion Matrix\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d', ax=ax) # Display the confusion matrix\n",
    "plt.title(\"Confusion Matrix - Final Test Set\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_confusion_matrix(cm, labels=[\"<=50K\", \">50K\"]):\n",
    "    tn, fp, fn, tp = cm.ravel()  # Unpack confusion matrix\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Confusion Matrix Interpretation – Final Test Set\")\n",
    "    interpretation.append(f\"- True Negatives ({labels[0]} predicted correctly): {tn}\")\n",
    "    interpretation.append(f\"- False Positives ({labels[0]} misclassified as {labels[1]}): {fp}\")\n",
    "    interpretation.append(f\"- False Negatives ({labels[1]} misclassified as {labels[0]}): {fn}\")\n",
    "    interpretation.append(f\"- True Positives ({labels[1]} predicted correctly): {tp}\")\n",
    "    # Optional derived metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    interpretation.append(\"\\nKey Metrics\")\n",
    "    interpretation.append(f\"- Accuracy: {accuracy:.2f}\")\n",
    "    interpretation.append(f\"- Precision: {precision:.2f}\")\n",
    "    interpretation.append(f\"- Recall: {recall:.2f}\")\n",
    "    interpretation.append(f\"- F1 Score: {f1:.2f}\")\n",
    "    # Bias comment\n",
    "    if recall > precision + 0.1:\n",
    "        interpretation.append(\"\\nThe model is more recall-focused, meaning it catches more of the >50K cases, but may include more false positives.\")\n",
    "    elif precision > recall + 0.1:\n",
    "        interpretation.append(\"\\nThe model is more precision-focused, meaning it's careful when predicting >50K but may miss many true >50K cases.\")\n",
    "    else:\n",
    "        interpretation.append(\"\\nThe model has a balanced precision-recall tradeoff.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_confusion_matrix(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c96fe",
   "metadata": {},
   "source": [
    "### Per-Class Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fac915",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['<=50K', '>50K']\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [cr[str(cls)][metric] for cls in [0, 1]]\n",
    "    plt.bar([x + i*0.25 for x in range(len(classes))], values, width=0.25, label=metric)\n",
    "plt.xticks([x + 0.25 for x in range(len(classes))], classes)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Per-Class Evaluation Metrics\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_per_class_metrics(cr, labels=[\"<=50K\", \">50K\"]):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Per-Class Evaluation Metrics Interpretation\")\n",
    "    for i, label in enumerate(labels):\n",
    "        class_report = cr[str(i)]\n",
    "        precision = class_report.get(\"precision\", 0)\n",
    "        recall = class_report.get(\"recall\", 0)\n",
    "        f1 = class_report.get(\"f1-score\", 0)\n",
    "        interpretation.append(f\"\\nClass: {label}\")\n",
    "        interpretation.append(f\"- Precision: {precision:.2f}\")\n",
    "        interpretation.append(f\"- Recall: {recall:.2f}\")\n",
    "        interpretation.append(f\"- F1 Score: {f1:.2f}\")\n",
    "        if recall > precision + 0.1:\n",
    "            interpretation.append(\"  The model is more focused on capturing this class, but may include more false positives.\")\n",
    "        elif precision > recall + 0.1:\n",
    "            interpretation.append(\"  The model is more conservative when predicting this class, possibly missing valid cases.\")\n",
    "        else:\n",
    "            interpretation.append(\"  The model shows a good balance between precision and recall.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_per_class_metrics(cr, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa33659",
   "metadata": {},
   "source": [
    "### Plotting the Confusion Matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29383b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr_df = cr_df.drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore') # Drop 'accuracy', 'macro avg', 'weighted avg' if needed\n",
    "cr_df = cr_df.round(2) # Optional: round to 2 decimals for cleaner heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(cr_df.iloc[:, :-1], annot=True, cmap='Blues', cbar=True, fmt='.2f')\n",
    "plt.title(\"Classification Report (Heatmap)\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_classification_report(cr, labels=[\"0\", \"1\"]):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Classification Report Summary\")\n",
    "    # Individual class performance\n",
    "    for label in labels:\n",
    "        precision = cr[label][\"precision\"]\n",
    "        recall = cr[label][\"recall\"]\n",
    "        f1 = cr[label][\"f1-score\"]\n",
    "        interpretation.append(f\"\\nClass {label} Metrics:\")\n",
    "        interpretation.append(f\"- Precision: {precision:.2f}\")\n",
    "        interpretation.append(f\"- Recall: {recall:.2f}\")\n",
    "        interpretation.append(f\"- F1 Score: {f1:.2f}\")\n",
    "        if recall > precision + 0.1:\n",
    "            interpretation.append(\"  The model focuses more on identifying this class, but it may generate more false positives.\")\n",
    "        elif precision > recall + 0.1:\n",
    "            interpretation.append(\"  The model is cautious in predicting this class, possibly missing true positives.\")\n",
    "        else:\n",
    "            interpretation.append(\"  Precision and recall are balanced for this class.\")\n",
    "    # Overall performance\n",
    "    interpretation.append(\"\\nOverall Metrics:\")\n",
    "    accuracy = cr[\"accuracy\"]\n",
    "    macro_avg = cr[\"macro avg\"]\n",
    "    weighted_avg = cr[\"weighted avg\"]\n",
    "    interpretation.append(f\"- Accuracy: {accuracy:.2f}\")\n",
    "    interpretation.append(f\"- Macro Avg Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1 Score: {macro_avg['f1-score']:.2f}\")\n",
    "    interpretation.append(f\"- Weighted Avg Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1 Score: {weighted_avg['f1-score']:.2f}\")\n",
    "    # Class imbalance comment\n",
    "    if abs(cr[\"0\"][\"recall\"] - cr[\"1\"][\"recall\"]) > 0.15:\n",
    "        interpretation.append(\"\\nThere may be class imbalance or difficulty in predicting one class compared to the other.\")\n",
    "    else:\n",
    "        interpretation.append(\"\\nThe model handles both classes fairly evenly.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_classification_report(cr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c912c74",
   "metadata": {},
   "source": [
    "### ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c965ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_probs)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {test_auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Final Test Set\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_roc_curve(test_auc):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"ROC Curve Interpretation – Final Test Set\")\n",
    "    interpretation.append(f\"- The Area Under the ROC Curve (AUC) is {test_auc:.4f}.\")\n",
    "    if test_auc >= 0.90:\n",
    "        interpretation.append(\"  This is considered excellent. The model has a strong ability to distinguish between the two classes.\")\n",
    "    elif test_auc >= 0.80:\n",
    "        interpretation.append(\"  This is considered very good. The model performs well in differentiating between the two classes.\")\n",
    "    elif test_auc >= 0.70:\n",
    "        interpretation.append(\"  This is considered fair. The model has a moderate level of separability.\")\n",
    "    else:\n",
    "        interpretation.append(\"  The model has poor separability and may not generalize well.\")\n",
    "    interpretation.append(\"  The curve bows well above the diagonal line, indicating that the model performs significantly better than random guessing.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_roc_curve(test_auc))  # Replace with your actual AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cfa57",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38119619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.step(recall, precision, color='b', alpha=0.8, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(f'Precision-Recall Curve: AP={average_precision:.3f}')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "baseline = len(y_test[y_test==1]) / len(y_test)  # No skill: positive class proportion\n",
    "plt.axhline(y=baseline, color='r', linestyle='--', label=f'Baseline: {baseline:.3f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Average Precision Score: {average_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ed51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Interpretation\n",
    "def interpret_precision_recall_curve(average_precision, baseline):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Precision-Recall Curve Interpretation – Final Test Set\")\n",
    "    interpretation.append(f\"- Average Precision (AP) score is {average_precision:.3f}.\")\n",
    "    interpretation.append(f\"- Baseline (positive class ratio) is approximately {baseline:.3f}.\")\n",
    "    if average_precision >= 0.8:\n",
    "        interpretation.append(\"  The model maintains high precision across a wide range of recall values, indicating strong performance.\")\n",
    "    elif average_precision >= 0.7:\n",
    "        interpretation.append(\"  The model performs well, with a good balance between precision and recall.\")\n",
    "    elif average_precision >= 0.6:\n",
    "        interpretation.append(\"  The model is acceptable, but may need tuning to improve precision or recall.\")\n",
    "    else:\n",
    "        interpretation.append(\"  The model struggles to maintain precision as recall increases.\")\n",
    "    if average_precision > baseline + 0.3:\n",
    "        interpretation.append(\"  The model performs significantly better than a no-skill classifier.\")\n",
    "    elif average_precision > baseline + 0.1:\n",
    "        interpretation.append(\"  The model shows improvement over baseline, but may need optimization.\")\n",
    "    else:\n",
    "        interpretation.append(\"  The model performs only slightly better than a no-skill classifier.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "print(interpret_precision_recall_curve(average_precision, baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba346b2",
   "metadata": {},
   "source": [
    "## Model Interpretation & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3205a",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "Based on our exploratory data analysis and model results, we can draw several conclusions:\n",
    "- **Education level** is strongly correlated with income, with higher education generally leading to higher income.\n",
    "- **Hours worked per week** shows a positive correlation with income—those earning >$50K tend to work more hours.\n",
    "- **Occupation categories** such as 'Executive-Managerial' and 'Professional-Specialty' are associated with higher income.\n",
    "- **Relationship status** plays a significant role, with 'Husband' having higher representation in the >$50K category.\n",
    "- **Age groups**: Middle-aged individuals are more likely to earn >$50K than young or old individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331bc1c",
   "metadata": {},
   "source": [
    "### Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf97571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model_performance_summary(test_auc, test_accuracy, test_precision, test_recall, cv_df=None, number_of_folds=None, context=\"high-income prediction\"):\n",
    "    interpretation = []\n",
    "    interpretation.append(\"Model Performance Summary\")\n",
    "    # ----- AUC -----\n",
    "    if test_auc >= 0.90:\n",
    "        interpretation.append(f\"- Excellent AUC: {test_auc:.2f} indicates strong separation between income classes.\")\n",
    "    elif test_auc >= 0.80:\n",
    "        interpretation.append(f\"- Strong AUC: {test_auc:.2f} reflects good discriminative ability.\")\n",
    "    elif test_auc >= 0.70:\n",
    "        interpretation.append(f\"- Moderate AUC: {test_auc:.2f} shows reasonable separation between classes.\")\n",
    "    else:\n",
    "        interpretation.append(f\"- Weak AUC: {test_auc:.2f} suggests limited ability to distinguish between classes.\")\n",
    "    # ----- Accuracy -----\n",
    "    if test_accuracy >= 0.85:\n",
    "        interpretation.append(f\"- High Accuracy: {test_accuracy:.2f} suggests the model predicts most cases correctly.\")\n",
    "    elif test_accuracy >= 0.75:\n",
    "        interpretation.append(f\"- Reasonable Accuracy: {test_accuracy:.2f} indicates solid overall correctness.\")\n",
    "    elif test_accuracy >= 0.65:\n",
    "        interpretation.append(f\"- Moderate Accuracy: {test_accuracy:.2f} shows acceptable performance.\")\n",
    "    else:\n",
    "        interpretation.append(f\"- Low Accuracy: {test_accuracy:.2f} means predictions are often incorrect.\")\n",
    "    # ----- Precision -----\n",
    "    if test_precision >= 0.80:\n",
    "        interpretation.append(f\"- Very High Precision: {test_precision:.2f} means most predicted positives are correct.\")\n",
    "    elif test_precision >= 0.60:\n",
    "        interpretation.append(f\"- Good Precision: {test_precision:.2f} reflects fairly reliable positive predictions.\")\n",
    "    elif test_precision >= 0.45:\n",
    "        interpretation.append(f\"- Moderate Precision: {test_precision:.2f} suggests some false positives are expected.\")\n",
    "    else:\n",
    "        interpretation.append(f\"- Low Precision: {test_precision:.2f} means many predicted positives may be incorrect.\")\n",
    "    # ----- Recall -----\n",
    "    if test_recall >= 0.90:\n",
    "        interpretation.append(f\"- Excellent Recall: {test_recall:.2f} shows the model captures nearly all actual positives.\")\n",
    "    elif test_recall >= 0.75:\n",
    "        interpretation.append(f\"- Strong Recall: {test_recall:.2f} means the model successfully detects most positive cases.\")\n",
    "    elif test_recall >= 0.60:\n",
    "        interpretation.append(f\"- Moderate Recall: {test_recall:.2f} indicates it misses some true positives.\")\n",
    "    else:\n",
    "        interpretation.append(f\"- Low Recall: {test_recall:.2f} shows the model is missing many actual positive cases.\")\n",
    "    # ----- Trade-off -----\n",
    "    interpretation.append(f\"- Precision vs Recall Trade-off: Precision = {test_precision:.2f}, Recall = {test_recall:.2f}\")\n",
    "    if test_recall > test_precision + 0.2:\n",
    "        interpretation.append(f\"  -> The model prioritizes catching more positives, even if some are incorrect. This is useful in {context} scenarios where false negatives are costly.\")\n",
    "    elif test_precision > test_recall + 0.2:\n",
    "        interpretation.append(f\"  -> The model is more cautious and precise, only flagging when confident. This may reduce false positives but miss actual cases.\")\n",
    "    else:\n",
    "        interpretation.append(f\"  -> The model maintains a balance between identifying positives and avoiding false positives.\")\n",
    "    # ----- Cross-validation Stability -----\n",
    "    if cv_df is not None and number_of_folds is not None:\n",
    "        stds = cv_df[['accuracy', 'precision', 'recall', 'auc']].std()\n",
    "        if all(stds < 0.02):\n",
    "            interpretation.append(f\"- Consistent Cross-Validation: The model maintained highly stable performance across all {number_of_folds} folds.\")\n",
    "        elif all(stds < 0.04):\n",
    "            interpretation.append(f\"- Reasonably Stable Cross-Validation: Low to moderate variance observed across folds.\")\n",
    "        else:\n",
    "            interpretation.append(f\"- Variable Cross-Validation: Significant variation across folds, suggesting sensitivity to train/val splits.\")\n",
    "    return \"\\n\".join(interpretation)\n",
    "# NOTE: This is IMPLEMENTED IN CONCLUSION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d5c5c",
   "metadata": {},
   "source": [
    "### Feature Importance Observations\n",
    "\n",
    "While we didn't explicitly calculate feature importance (e.g., using SHAP values), our EDA suggests these key predictors:\n",
    "\n",
    "1. Education level (number of years)\n",
    "2. Occupation type\n",
    "3. Hours worked per week\n",
    "4. Marital status/relationship\n",
    "5. Age group (Middle-aged > Old > Young)\n",
    "\n",
    "These insights align with common socio-economic understanding that education, occupation, and work experience strongly influence income levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d95f57",
   "metadata": {},
   "source": [
    "## Real-World Applications & Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146614e",
   "metadata": {},
   "source": [
    "### Use Cases in Financial Services\n",
    "\n",
    "This income prediction model has several practical applications in the financial sector:\n",
    "\n",
    "1. **Alternative Credit Scoring**: Supplement traditional credit scores for more inclusive lending, especially for \"thin file\" customers with limited credit history.\n",
    "2. **Risk Assessment**: Enhance loan default prediction by adding income stability as a factor.\n",
    "3. **Customer Segmentation**: Better tailor financial products based on predicted income brackets.\n",
    "4. **Fraud Detection**: Flag inconsistencies between stated and predicted income as potential indicators of fraud.\n",
    "5. **Financial Planning**: Help advisors understand client income potential for more personalized advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee6ba2",
   "metadata": {},
   "source": [
    "### Ethical Considerations\n",
    "\n",
    "While powerful, these models raise important ethical considerations:\n",
    "\n",
    "- **Fairness**: The model may perpetuate historical biases in income distribution across demographic groups.\n",
    "- **Privacy**: Using demographic data for financial decisions raises privacy concerns.\n",
    "- **Transparency**: Neural networks are \"black box\" models that may be difficult to explain to affected individuals.\n",
    "- **Regulatory Compliance**: Financial institutions must ensure such models comply with fair lending laws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368be9ca",
   "metadata": {},
   "source": [
    "### Limitations & Potential Improvements\n",
    "\n",
    "Several limitations should be addressed in future work:\n",
    "\n",
    "1. **Historical Data**: The dataset is from the 1990s and may not reflect current socioeconomic realities.\n",
    "2. **Feature Engineering**: More sophisticated feature engineering could improve performance.\n",
    "3. **Model Explainability**: Incorporating explainability techniques like SHAP would enhance transparency.\n",
    "4. **Geographic Context**: Income significance varies by region; geographic normalization could help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f853f0",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6366",
   "metadata": {},
   "source": [
    "### Reflect on the challenges faced during the project and how they were addressed.\n",
    "Several Challenges were encountered during this project were:\n",
    "- Understanding How to Implemenet Neural Network from Start.\n",
    "- Figure out what Relevant Visulisations can be Added.\n",
    "- Figuring out how to use a Validation set so that i am not Overfitting the Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e35a0",
   "metadata": {},
   "source": [
    "### Discuss any insights gained from applying the selected method.\n",
    "Insights Gained were encountered during this project were:\n",
    "- How to Train Neural Networks.\n",
    "- How to Explore the Practicality of a Dataset.\n",
    "- Making Sure that the Model isn't overfitting to the Training Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d142e8f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully developed and validated a neural network model for income classification using the UCI Census dataset. The results and implications can be summarized in several key areas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab51efb",
   "metadata": {},
   "source": [
    "\n",
    "### Key Technical Achievements\n",
    "- Successfully implemented a deep learning architecture with appropriate regularization techniques (dropout, batch normalization).\n",
    "- Effectively handled class imbalance through class weighting.\n",
    "- Achieved good model convergence as evidenced by the learning curves.\n",
    "\n",
    "### Business Impact & Applications\n",
    "The model shows particular promise for:\n",
    "- Risk assessment in lending decisions\n",
    "- Customer segmentation in financial services\n",
    "- Automated pre-screening in loan applications\n",
    "\n",
    "### Limitations & Future Work\n",
    "While successful, several areas could be improved:\n",
    "1. More sophisticated feature engineering could potentially improve performance\n",
    "2. Integration of geographic cost-of-living data could provide better context\n",
    "3. Implementation of explainability techniques (e.g., SHAP values) would enhance transparency\n",
    "\n",
    "### Final Thoughts\n",
    "The project demonstrates that machine learning, specifically neural networks, can effectively predict income levels from demographic and employment data with high reliability. While not perfect, the model's performance makes it a valuable tool for initial risk assessment in financial applications, provided it's used as part of a broader decision-making framework that considers other relevant factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_model_performance_summary(\n",
    "    test_auc=test_auc,\n",
    "    test_accuracy=test_accuracy,\n",
    "    test_precision=test_precision,\n",
    "    test_recall=test_recall,\n",
    "    cv_df=df_metrics,  # Optional\n",
    "    number_of_folds=number_of_folds,  # Optional\n",
    "    context=\"high-income prediction\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdb8db",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. UCI Machine Learning Repository: [Census Income Dataset](https://archive.ics.uci.edu/dataset/20/census+income)  \n",
    "2. Scikit-learn: [Machine Learning in Python](https://scikit-learn.org/)  \n",
    "3. TensorFlow: [Keras API Guide](https://www.tensorflow.org/guide/keras)  \n",
    "4. Graphviz: [Graphviz Download & Install](https://graphviz.org/)  \n",
    "5. Matplotlib: [Visualization with Python](https://matplotlib.org/stable/)  \n",
    "6. Seaborn: [Statistical Data Visualization](https://seaborn.pydata.org/)  \n",
    "7. Pandas: [Python Data Analysis Library](https://pandas.pydata.org/docs/)  \n",
    "8. NumPy: [NumPy Reference](https://numpy.org/doc/)  \n",
    "9. GeeksforGeeks: \n",
    "    - [Handling Imbalanced Data](https://www.geeksforgeeks.org/handling-imbalanced-data-for-classification/)  \n",
    "    - [K-Fold Cross-Validation](https://www.geeksforgeeks.org/cross-validation-machine-learning/)  \n",
    "10. Medium: [Train, Validation, Test Splits](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
