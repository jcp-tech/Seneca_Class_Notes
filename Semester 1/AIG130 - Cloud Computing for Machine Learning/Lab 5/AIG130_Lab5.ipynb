{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AIG130 – Lab 5: AutoML]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn kagglehub ucimlrepo google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv # Used for Environment Variables\n",
    "# import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "# import math\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset & Ignore warnings\n",
    "from ucimlrepo import fetch_ucirepo # , list_available_datasets\n",
    "import kagglehub\n",
    "import warnings\n",
    "\n",
    "from google.cloud.aiplatform.v1.schema.trainingjob.definition_v1.types import AutoMlTablesInputs\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I'm loading the Environment Variables from the .env file, if the file is not found then the code will stop execution.\n",
    "try:\n",
    "    if not load_dotenv():\n",
    "        # raise FileNotFoundError(\".env file not found\")\n",
    "        sys.exit(\"Stopping execution as environment variables are required\")\n",
    "    # else:\n",
    "    #     load_dotenv()\n",
    "    #     # print(\".env file loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading .env file: {e}\")\n",
    "    # raise SystemExit(\"Stopping execution as environment variables are required\")\n",
    "    sys.exit(\"Stopping execution as environment variables are required\")\n",
    "else:\n",
    "    if os.getenv(\"API_KEY\") is None:\n",
    "        sys.exit(\"Stopping execution as required environment variables are not set\")\n",
    "finally:\n",
    "    load_dotenv()\n",
    "    print(\"Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_file = r\"C:\\Users\\JonathanChackoPattas\\OneDrive - Maritime Support Solutions\\Desktop\\Class Notes\\Seneca\\Semester 1\\AIG130 - Cloud Computing for Machine Learning\\Lab 5\\ignore\\serviceAccountKey.json\" # r'I:\\Work\\MSS-Automation\\Connectors\\serviceAccountKey.json'\n",
    "\n",
    "def set_gc_credentials(service_account_file):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_account_file # if os.path.exists(service_account_file) else os.environ.get('GC_CREDS') if 'GC_CREDS' in os.environ and os.environ.get('GC_CREDS') else None\n",
    "\n",
    "set_gc_credentials(service_account_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.getenv(\"PROJECT_ID\") # @param {type:\"string\"}\n",
    "LOCATION = os.getenv(\"LOCATION\") # @param {type:\"string\"}\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\") # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "# Upload CSV to Google Cloud Storage (GCS)\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcp_process_data(df, target, bucket, proccess_type =\"classification\"):\n",
    "    \n",
    "    DATASET_NAME = f\"{proccess_type}-dataset\" # @param {type:\"string\"}\n",
    "\n",
    "    # Split into Train and Test\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Save Pandas DataFrame as CSV\n",
    "    train_data.to_csv(f\"{proccess_type}_data.csv\", index=False)\n",
    "\n",
    "    blob = bucket.blob(f\"{proccess_type}_data.csv\")\n",
    "    blob.upload_from_filename(f\"{proccess_type}_data.csv\")\n",
    "\n",
    "    # GCS Path\n",
    "    gcs_uri = f\"gs://{bucket.name}/{proccess_type}_data.csv\"\n",
    "\n",
    "    # Create Vertex AI Dataset\n",
    "    dataset = aiplatform.TabularDataset.create(\n",
    "        display_name=DATASET_NAME,\n",
    "        gcs_source=[gcs_uri]\n",
    "    )\n",
    "\n",
    "    # dataset.resource_name # to PRINT\n",
    "\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name=\"automl-tabular-model\",\n",
    "        optimization_prediction_type=proccess_type,\n",
    "        # column_transformations=[\n",
    "        #     {\"\": {\"column_name\": \"\"}}, # {\"numeric\": {\"column_name\": \"Age\"}},\n",
    "        # ]\n",
    "    )\n",
    "\n",
    "    model = job.run(\n",
    "        dataset=dataset,\n",
    "        target_column=target,\n",
    "        # training_fraction_split=0.8,\n",
    "        # validation_fraction_split=0.1,\n",
    "        # test_fraction_split=0.1,\n",
    "        model_display_name=\"adopted-prediction-model\",\n",
    "        # disable_early_stopping=False,\n",
    "    )\n",
    "\n",
    "    # Deploy the Model\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "\n",
    "    test_data.drop(columns=[target], inplace=True)\n",
    "    \n",
    "    predictions = endpoint.predict(instances=test_data.to_dict(orient=\"records\"))\n",
    "    \n",
    "    print(predictions)\n",
    "    \n",
    "    # # Warning: Setting this to true will delete everything in your bucket\n",
    "    # delete_bucket = False\n",
    "\n",
    "    # # Delete the training job\n",
    "    # job.delete()\n",
    "\n",
    "    # # Delete the model\n",
    "    # model.delete()\n",
    "\n",
    "    # # Delete the endpoint\n",
    "    # endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task\n",
    "**Dataset**: Flight Price Prediction dataset\n",
    "\n",
    "**Source**: [Kaggle](https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction)\n",
    "\n",
    "**Objective**: To predict the price of airline tickets based on various features such as airline, flight details, source and destination cities, departure and arrival times, number of stops, class, duration, and days left before the flight. This regression model will help travelers anticipate flight costs and potentially make more economical travel decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  df_regression = kagglehub.load_dataset(\n",
    "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
    "    \"shubhambathwal/flight-price-prediction\",\n",
    "    \"Clean_Dataset.csv\",\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(f\"Error loading dataset: {e}\")\n",
    "  path = kagglehub.dataset_download(\"shubhambathwal/flight-price-prediction\") # Download latest version\n",
    "  df_regression = pd.read_csv(path+\"/Clean_Dataset.csv\")\n",
    "finally:\n",
    "  X = df_regression.drop(columns=['price']) # Features\n",
    "  y = df_regression['price'] # Target variable\n",
    "  # df_regression = pd.concat([X, y], axis=1) # concatenate features and target variable\n",
    "  df_regression.drop(columns=['Unnamed: 0'], inplace=True) # Drop the unnessary index column\n",
    "df_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells about the data types and their distribution\n",
    "df_regression.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells about the number of rows and columns in the dataset\n",
    "df_regression.shape # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells about the data types of each column\n",
    "df_regression.info() # df_regression.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells us if there are any missing values in the dataset\n",
    "print(\"Missing values in the dataset:\")\n",
    "int(df_regression.isnull().sum().sum()) # There aren't any missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine class distribution of the target variable price\n",
    "plt.figure(figsize=(8, 6)) # (20, 15)\n",
    "sns.histplot(df_regression['price'], kde=True)\n",
    "plt.title('Price Distribution of Houses')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Bar Distribution of Categorical Variables in one figure\n",
    "categorical_cols = df_regression.select_dtypes(include='object').columns\n",
    "n_cols = len(categorical_cols)\n",
    "n_rows = (n_cols + 1) // 2  # Calculate number of rows needed (2 columns per row)\n",
    "# Create a color palette using a different scheme\n",
    "colors = sns.color_palette(\"Set2\", 8)  # Using Set2 palette instead of husl\n",
    "plt.figure(figsize=(15, 4*n_rows))\n",
    "for idx, col in enumerate(categorical_cols, 1):\n",
    "    plt.subplot(n_rows, 2, idx)\n",
    "    data = df_regression[col].value_counts()\n",
    "    data = data.head(8) if len(data) > 8 else data  # Display only top 8 categories if there are more than 8 categories\n",
    "    sns.barplot(x=data.index, y=data.values, palette=colors)\n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "# # Display value counts for categorical variables\n",
    "# for col in df_regression.select_dtypes(include='object').columns:\n",
    "#     print(f\"Column: {col}\")\n",
    "#     print(df_regression[col].value_counts())\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.plotting import scatter_matrix\n",
    "numerical_cols = df_regression.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(12, 12))\n",
    "n = len(numerical_cols)\n",
    "colors = sns.color_palette(\"husl\", n*n)\n",
    "cidx = 0\n",
    "for i in range(n):\n",
    "    for j in range(i+1):  # Changed this line to create triangle\n",
    "        plt.subplot(n, n, i*n + j + 1)\n",
    "        plt.scatter(\n",
    "            df_regression[numerical_cols[j]], df_regression[numerical_cols[i]], \n",
    "            alpha=0.5, c=colors[cidx], s=10\n",
    "        )\n",
    "        cidx += 1\n",
    "        if i == n-1:\n",
    "            plt.xlabel(numerical_cols[j])\n",
    "        if j == 0:\n",
    "            plt.ylabel(numerical_cols[i])\n",
    "plt.suptitle('Scatter Plot Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "# scatter_matrix(df_regression, figsize=(20, 20))\n",
    "# plt.title('Scatter Matrix of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_regression = gcp_process_data(df_regression, 'price', bucket, proccess_type=\"regression\")\n",
    "prediction_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task\n",
    "**Dataset**: Phishing Websites dataset\n",
    "\n",
    "**Source**: [UCI ML Repository](https://archive.ics.uci.edu/dataset/327/phishing+websites)\n",
    "\n",
    "**Objective**: To classify websites as either legitimate or phishing based on various URL and website features. This classification model will help improve cybersecurity by automatically identifying potentially malicious websites that attempt to steal sensitive information from users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # fetch dataset \n",
    "    phishing_websites = fetch_ucirepo(id=327) \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    phishing_websites = fetch_ucirepo(name='Phishing Websites') # fetch dataset by name\n",
    "finally:\n",
    "    X =  pd.DataFrame(phishing_websites.data.features) # Features | phishing_websites.data.features\n",
    "    y =  pd.DataFrame(phishing_websites.data.targets) # Target variable | phishing_websites.data.targets\n",
    "    df_classification = pd.concat([X, y], axis=1) # concatenate features and target variable\n",
    "df_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification.info() # df_classification.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells us if there are any missing values in the dataset\n",
    "print(\"Missing values in the dataset:\")\n",
    "int(df_classification.isnull().sum().sum()) # There aren't any missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine class distribution of the target variable\n",
    "class_dist = df_classification['result'].value_counts()\n",
    "# Calculate percentages\n",
    "total = len(df_classification)\n",
    "percentages = (class_dist / total * 100).round(1)\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(class_dist, labels=['Legitimate', 'Phishing'], \n",
    "    autopct=lambda pct: f'{pct:.1f}%',\n",
    "    colors=['lightgreen', 'lightcoral'])\n",
    "plt.title('Class Distribution in Phishing Websites Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = [ftp for ftp in df_classification.columns if ftp != 'result'] # [ftp[0] for ftp in df_classification.corr().loc['result'].abs().sort_values(ascending=False).items() if ftp[0] != 'result']\n",
    "n_cols = 2 if len(features_to_plot) % 2 == 0 else 3\n",
    "n_rows = (len(features_to_plot) + 1) // n_cols\n",
    "plt.figure(figsize=(15, 4*n_rows))\n",
    "for idx, feature in enumerate(features_to_plot, 1):\n",
    "    plt.subplot(n_rows, n_cols, idx)\n",
    "    sns.countplot(data=df_classification, x=feature, hue='result')\n",
    "    plt.title(f'{feature} Distribution by Class')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Result', labels=['Phishing', 'Legitimate'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# # Display value counts for categorical variables\n",
    "# for col in df_classification.select_dtypes(include='object').columns:\n",
    "#     print(f\"Column: {col}\")\n",
    "#     print(df_classification[col].value_counts())\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = df_classification.corr()\n",
    "# Create mask for upper triangle to avoid redundancy\n",
    "mask = np.triu(np.ones_like(correlation_matrix))\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 16))\n",
    "# Create heatmap with better visualization\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True,  # Show correlation values\n",
    "            fmt='.2f',   # Round to 2 decimal places\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": .5},\n",
    "            annot_kws={\"size\": 8})\n",
    "plt.title('Feature Correlation Heatmap', pad=20, size=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_classification = gcp_process_data(df_classification, 'result', bucket, proccess_type=\"classification\")\n",
    "prediction_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Scikit-learn Column Transformer Documentation](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)\n",
    "2. [UCI Machine Learning Repository: Phishing Websites Dataset](https://archive.ics.uci.edu/dataset/327/phishing+websites)\n",
    "3. [Kaggle: Flight Price Prediction Dataset](https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction)\n",
    "4. [Scikit-learn Documentation: Regression Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "5. [Scikit-learn Documentation: Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "6. [Random Forest Algorithm: Theory and Applications](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n",
    "7. [IPYNB Reference](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl-tabular-classification.ipynb)\n",
    "8. [AIG100 – Project 2: Regression and Classification Methods](https://github.com/jcp-tech/Seneca_Class_Notes/tree/master/Semester%201/AIG100%20-%20Machine%20Learing/Project%202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
