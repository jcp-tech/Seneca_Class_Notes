{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AIG130 – Lab 5: AutoML](https://github.com/jcp-tech/Seneca_Class_Notes/tree/master/Semester%201/AIG130%20-%20Cloud%20Computing%20for%20Machine%20Learning/Lab%205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn kagglehub ucimlrepo google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "# import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv # Used for Environment Variables\n",
    "# import scipy.stats as stats\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import sys, os\n",
    "# import math\n",
    "\n",
    "## Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "## Load the dataset & Ignore warnings\n",
    "from ucimlrepo import fetch_ucirepo # , list_available_datasets\n",
    "import kagglehub\n",
    "import warnings\n",
    "\n",
    "## GCP Libraries\n",
    "from google.cloud import aiplatform # AutoML from Vertex AI\n",
    "from google.cloud import storage # Buckets in GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I'm loading the Environment Variables from the .env file, if the file is not found then the code will stop execution.\n",
    "try:\n",
    "    if not load_dotenv():\n",
    "        # raise FileNotFoundError(\".env file not found\")\n",
    "        sys.exit(\"Stopping execution as environment variables are required\")\n",
    "    # else:\n",
    "    #     load_dotenv()\n",
    "    #     # print(\".env file loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading .env file: {e}\")\n",
    "    # raise SystemExit(\"Stopping execution as environment variables are required\")\n",
    "    sys.exit(\"Stopping execution as environment variables are required\")\n",
    "else:\n",
    "    if os.getenv(\"API_KEY\") is None:\n",
    "        sys.exit(\"Stopping execution as required environment variables are not set\")\n",
    "finally:\n",
    "    load_dotenv()\n",
    "    print(\"Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_file = r\"C:\\Users\\JonathanChackoPattas\\OneDrive - Maritime Support Solutions\\Desktop\\Class Notes\\Seneca\\Semester 1\\AIG130 - Cloud Computing for Machine Learning\\Lab 5\\ignore\\serviceAccountKey.json\" # r'I:\\Work\\MSS-Automation\\Connectors\\serviceAccountKey.json'\n",
    "\n",
    "def set_gc_credentials(service_account_file):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = service_account_file # if os.path.exists(service_account_file) else os.environ.get('GC_CREDS') if 'GC_CREDS' in os.environ and os.environ.get('GC_CREDS') else None\n",
    "\n",
    "set_gc_credentials(service_account_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.getenv(\"PROJECT_ID\") # @param {type:\"string\"}\n",
    "LOCATION = os.getenv(\"LOCATION\") # @param {type:\"string\"}\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\") # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcp_process_data(df, target, bucket, proccess_type =\"classification\"):\n",
    "    \n",
    "    DATASET_NAME = f\"{proccess_type}-dataset\" # @param {type:\"string\"}\n",
    "\n",
    "    # Split into Train and Test\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Save Pandas DataFrame as CSV\n",
    "    train_data.to_csv(f\"{proccess_type}_data.csv\", index=False)\n",
    "\n",
    "    blob = bucket.blob(f\"{proccess_type}_data.csv\")\n",
    "    blob.upload_from_filename(f\"{proccess_type}_data.csv\")\n",
    "\n",
    "    # GCS Path\n",
    "    gcs_uri = f\"gs://{bucket.name}/{proccess_type}_data.csv\"\n",
    "\n",
    "    # Create Vertex AI Dataset\n",
    "    dataset = aiplatform.TabularDataset.create(\n",
    "        display_name=DATASET_NAME,\n",
    "        gcs_source=[gcs_uri]\n",
    "    )\n",
    "\n",
    "    # dataset.resource_name # to PRINT\n",
    "\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name=\"automl-tabular-model\",\n",
    "        optimization_prediction_type=proccess_type,\n",
    "        # column_transformations=[\n",
    "        #     {\"\": {\"column_name\": \"\"}}, # {\"numeric\": {\"column_name\": \"Age\"}},\n",
    "        # ]\n",
    "    )\n",
    "\n",
    "    model = job.run(\n",
    "        dataset=dataset,\n",
    "        target_column=target,\n",
    "        # training_fraction_split=0.8,\n",
    "        # validation_fraction_split=0.1,\n",
    "        # test_fraction_split=0.1,\n",
    "        model_display_name=\"adopted-prediction-model\",\n",
    "        # disable_early_stopping=False,\n",
    "    )\n",
    "\n",
    "    # Deploy the Model\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "\n",
    "    test_data.drop(columns=[target], inplace=True)\n",
    "    \n",
    "    predictions = endpoint.predict(instances=test_data.to_dict(orient=\"records\"))\n",
    "\n",
    "    # Delete the training job\n",
    "    job.delete()\n",
    "\n",
    "    # Delete the model\n",
    "    model.delete()\n",
    "\n",
    "    # Delete the endpoint\n",
    "    endpoint.delete()\n",
    "\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task\n",
    "**Dataset**: Flight Price Prediction dataset\n",
    "\n",
    "**Source**: [Kaggle](https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction)\n",
    "\n",
    "**Objective**: To predict the price of airline tickets based on various features such as airline, flight details, source and destination cities, departure and arrival times, number of stops, class, duration, and days left before the flight. This regression model will help travelers anticipate flight costs and potentially make more economical travel decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  df_regression = kagglehub.load_dataset(\n",
    "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
    "    \"shubhambathwal/flight-price-prediction\",\n",
    "    \"Clean_Dataset.csv\",\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(f\"Error loading dataset: {e}\")\n",
    "  path = kagglehub.dataset_download(\"shubhambathwal/flight-price-prediction\") # Download latest version\n",
    "  df_regression = pd.read_csv(path+\"/Clean_Dataset.csv\")\n",
    "finally:\n",
    "  X = df_regression.drop(columns=['price']) # Features\n",
    "  y = df_regression['price'] # Target variable\n",
    "  # df_regression = pd.concat([X, y], axis=1) # concatenate features and target variable\n",
    "  df_regression.drop(columns=['Unnamed: 0'], inplace=True) # Drop the unnessary index column\n",
    "df_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_regression = gcp_process_data(df_regression, 'price', bucket, proccess_type=\"regression\")\n",
    "prediction_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task\n",
    "**Dataset**: Phishing Websites dataset\n",
    "\n",
    "**Source**: [UCI ML Repository](https://archive.ics.uci.edu/dataset/327/phishing+websites)\n",
    "\n",
    "**Objective**: To classify websites as either legitimate or phishing based on various URL and website features. This classification model will help improve cybersecurity by automatically identifying potentially malicious websites that attempt to steal sensitive information from users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # fetch dataset \n",
    "    phishing_websites = fetch_ucirepo(id=327) \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    phishing_websites = fetch_ucirepo(name='Phishing Websites') # fetch dataset by name\n",
    "finally:\n",
    "    X =  pd.DataFrame(phishing_websites.data.features) # Features | phishing_websites.data.features\n",
    "    y =  pd.DataFrame(phishing_websites.data.targets) # Target variable | phishing_websites.data.targets\n",
    "    df_classification = pd.concat([X, y], axis=1) # concatenate features and target variable\n",
    "df_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_classification = gcp_process_data(df_classification, 'result', bucket, proccess_type=\"classification\")\n",
    "prediction_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [UCI Machine Learning Repository: Phishing Websites Dataset](https://archive.ics.uci.edu/dataset/327/phishing+websites)\n",
    "2. [Kaggle: Flight Price Prediction Dataset](https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction)\n",
    "3. [Random Forest Algorithm: Theory and Applications](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n",
    "4. [IPYNB Reference](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl-tabular-classification.ipynb)\n",
    "5. [AIG100 – Project 2: Regression and Classification Methods](https://github.com/jcp-tech/Seneca_Class_Notes/tree/master/Semester%201/AIG100%20-%20Machine%20Learing/Project%202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
