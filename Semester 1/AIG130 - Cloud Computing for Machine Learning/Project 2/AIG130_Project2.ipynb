{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Job Posting Prediction Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for detecting fake job postings using HuggingFace Transformers and AutoML techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install transformers datasets pandas scikit-learn torch kagglehub tpot[sklearnex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion\n",
    "\n",
    "Download the job postings dataset from Kaggle and load it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import tpot\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "  df = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS, #   KaggleDatasetAdapter.HUGGING_FACE,\n",
    "    \"shivamb/real-or-fake-fake-jobposting-prediction\",\n",
    "    \"fake_job_postings.csv\"\n",
    "  )\n",
    "except Exception as e:\n",
    "  df = None\n",
    "  print(f\"Error loading dataset: {e}\")\n",
    "  path = kagglehub.dataset_download(\"shivamb/real-or-fake-fake-jobposting-prediction\")\n",
    "  # Try different encodings\n",
    "  encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "  for encoding in encodings:\n",
    "    try:\n",
    "      df = pd.read_csv(path + \"\\\\fake_job_postings.csv\", encoding=encoding)\n",
    "      print(f\"Successfully loaded with {encoding} encoding\")\n",
    "      break\n",
    "    except UnicodeDecodeError:\n",
    "      continue\n",
    "    except Exception as e:\n",
    "      print(f\"Error with {encoding} encoding: {e}\")\n",
    "      continue\n",
    "\n",
    "if df is None:\n",
    "  df = pd.read_csv(\"fake_job_postings.csv\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(f\"Missing values before cleaning:\\n{df.isnull().sum()}\\n\")\n",
    "\n",
    "# Fill missing text fields with empty strings\n",
    "text_columns = ['title', 'location', 'department', 'company_profile', 'description', \n",
    "               'requirements', 'benefits', 'employment_type', 'required_experience', \n",
    "               'required_education', 'industry', 'function']\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('')\n",
    "\n",
    "# Remove duplicates\n",
    "df_size_before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Removed {df_size_before - len(df)} duplicate entries\")\n",
    "\n",
    "# Create a consolidated text field for modeling\n",
    "df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['description'] + ' ' + \\\n",
    "            df['requirements'] + ' ' + df['benefits'] + ' ' + df['company_profile']\n",
    "\n",
    "# Basic text cleaning\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML, special chars, and extra whitespace\"\"\"\n",
    "    # Remove HTML\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to 100 samples to test (full cleaning in feature engineering)\n",
    "df.loc[:100, 'text'] = df.loc[:100, 'text'].apply(clean_text)\n",
    "\n",
    "# Convert target to numeric\n",
    "df['fraudulent'] = df['fraudulent'].astype(int)\n",
    "\n",
    "print(f\"Missing values after cleaning:\\n{df.isnull().sum()}\\n\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud distribution:\\n{df['fraudulent'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['fraudulent'], random_state=42)\n",
    "\n",
    "# Basic statistics and class distribution\n",
    "print(f\"Training set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"Training: {df_train['fraudulent'].value_counts(normalize=True)}\")\n",
    "print(f\"Testing: {df_test['fraudulent'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Visualization of class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "df['fraudulent'].value_counts().plot(kind='bar', title='Count of Job Postings by Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Fraudulent (1) vs Real (0)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['fraudulent'].value_counts(normalize=True).plot(kind='pie', \n",
    "                                                   autopct='%1.1f%%',\n",
    "                                                   labels=['Real', 'Fake'],\n",
    "                                                   title='Percentage of Real vs Fake Job Postings')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of text length vs fraud\n",
    "df['text_length'] = df['text'].str.len()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([df[df['fraudulent']==0]['text_length'], df[df['fraudulent']==1]['text_length']], \n",
    "         bins=50, alpha=0.7, label=['Real', 'Fake'])\n",
    "plt.legend()\n",
    "plt.title('Distribution of Text Length by Job Posting Type')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Analyze job requirements by fraud status\n",
    "plt.figure(figsize=(10, 6))\n",
    "req_length = df.groupby('fraudulent')['requirements'].apply(lambda x: x.str.len().mean())\n",
    "req_length.plot(kind='bar', title='Average Length of Requirements by Job Type')\n",
    "plt.xlabel('Fraudulent (1) vs Real (0)')\n",
    "plt.ylabel('Avg Length (characters)')\n",
    "plt.xticks([0, 1], ['Real', 'Fake'])\n",
    "plt.show()\n",
    "\n",
    "# Top locations for real vs fake jobs\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_real_locs = df[df['fraudulent']==0]['location'].value_counts().head(10)\n",
    "top_fake_locs = df[df['fraudulent']==1]['location'].value_counts().head(10)\n",
    "\n",
    "print(\"Top locations for real job postings:\")\n",
    "print(top_real_locs)\n",
    "print(\"\\nTop locations for fake job postings:\")\n",
    "print(top_fake_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## NOTE: TODO & TEST this Implementation.\n",
    "\n",
    "# Train the Model using TPOT\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = TPOTClassifier(verbosity=2, generations=5, population_size=20, random_state=42, config_dict='TPOT sparse')\n",
    "model.fit(df_train['text'], df_train['fraudulent'])\n",
    "\n",
    "print(f\"Best pipeline accuracy: {accuracy_score(df_test['fraudulent'], model.predict(df_test['text']))}\")\n",
    "print(classification_report(df_test['fraudulent'], model.predict(df_test['text'])))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
