{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24775f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'i', 'am', 'Jonathan', 'Chacko', 'Pattasseril', '.', 'I', 'am', 'learning', 'NLP', 'and', 'Python', 'programming', '.'] \n",
      "\n",
      "Using words.sort(): None\n",
      "The original list is modified in place and sorted: ['Hello', ',', 'i', 'am', 'Jonathan', 'Chacko', 'Pattasseril', '.', 'I', 'am', 'learning', 'NLP', 'and', 'Python', 'programming', '.']\n",
      "Stored in sort_type_1: None \n",
      "\n",
      "Using sorted(): [',', '.', '.', 'Chacko', 'Hello', 'I', 'Jonathan', 'NLP', 'Pattasseril', 'Python', 'am', 'am', 'and', 'i', 'learning', 'programming']\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "'''\n",
    "Create a variable words containing a list of words. Experiment with words.sort() and sorted(words) (and perhaps your list itself) until the difference in operation is clear. What is the difference?\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "sentence = \"Hello, i am Jonathan Chacko Pattasseril. I am learning NLP and Python programming.\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(words, \"\\n\")\n",
    "\n",
    "# Using words.sort()\n",
    "words_copy_1 = words.copy()\n",
    "sort_type_1 = words.sort()\n",
    "print(\"Using words.sort():\", words.sort())\n",
    "print(\"The original list is modified in place and sorted:\", words_copy_1)\n",
    "print(\"Stored in sort_type_1:\", sort_type_1, \"\\n\")\n",
    "\n",
    "# Using sorted()\n",
    "words_copy_2 = words.copy()\n",
    "sort_type_2 = sorted(words)\n",
    "print(\"Using sorted():\", sort_type_2)\n",
    "# The difference is that words.sort() sorts the list in place and returns None, while sorted(words) returns a new sorted list without modifying the original list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb57553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 3: Lowering Everyday Costs | Budget 2024 Skip to main content Skip to About this\n",
      "\t\t\t\t\tsite Language selection FranÃ§ais fr / Gouvernement du \n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "'''\n",
    "Load the text from a portion of the 2024 Federal Budget (located at https://budget.canada.ca/2024/report-rapport/chap3-en.html\". Store the plain text (with HTML tags/etc. removed) into a string. Print the first 150 characters of the string.\n",
    "'''\n",
    "\n",
    "import nltk, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "url = \"https://budget.canada.ca/2024/report-rapport/chap3-en.html\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "text = soup.get_text(separator=' ', strip=True)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "cleaned_text = ' '.join(sentences)\n",
    "print(cleaned_text[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8132d3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 punctuation tokens:\n",
      "[':', '|', '/', '.', '.', ',', ',', ':', '.', ':', ':', '.', '.', ',', '.', ',', ':', '.', ',', '.']\n",
      "\n",
      "Frequency distribution of punctuation symbols:\n",
      "\t: => 45\n",
      "\t| => 1\n",
      "\t/ => 2\n",
      "\t. => 447\n",
      "\t, => 432\n",
      "\t' => 47\n",
      "\t; => 31\n",
      "\t- => 221\n",
      "\t$ => 64\n",
      "\t( => 14\n",
      "\t) => 14\n",
      "\t= => 1\n",
      "\t* => 6\n",
      "\t+ => 1\n",
      "\t? => 1\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "'''\n",
    "Use nltk.regexp_tokenize() to tokenize the string you created in Question 2. Use a regexp so that only punctuation characters are tokenized (i.e., your regexp should match any (and only) punctuation). Some notes, tips:\n",
    "\n",
    "Please use the 'verbose' flag in your regexp. Starting your regexp with the verbose flag '(?x)' allows your regexp to be spread over multiple lines (with whitespace ignored), for increased readability. Enclose your regexp in triple quotes (\"\"\").\n",
    "Certain punctuation characters are also operators within the regexp (e.g., the period). You can include these characters as literals by escaping them with a preceding backslash. (E.g., \\. matches a period character, rather than matching any character.)\n",
    "\n",
    "\n",
    "Print the first 20 elements of the resulting string. Also, use FreqDist to count the frequency of each punctuation symbol.\n",
    "'''\n",
    "import nltk # , requests\n",
    "# from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "# url = \"https://budget.canada.ca/2024/report-rapport/chap3-en.html\"\n",
    "# response = requests.get(url)\n",
    "# html_content = response.text\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# text = soup.get_text(separator=' ', strip=True)\n",
    "# sentences = nltk.sent_tokenize(text)\n",
    "# cleaned_text = ' '.join(sentences)\n",
    "\n",
    "punctuation_regexp = r\"\"\"(?x)\n",
    "    [\\!\"#\\$%&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}~–—‘’“”•…]\n",
    "\"\"\"\n",
    "\n",
    "punctuation_tokens = regexp_tokenize(cleaned_text, punctuation_regexp)\n",
    "\n",
    "print(\"First 20 punctuation tokens:\")\n",
    "print(punctuation_tokens[:20])\n",
    "\n",
    "fdist_punctuation = FreqDist(punctuation_tokens)\n",
    "print(\"\\nFrequency distribution of punctuation symbols:\")\n",
    "for symbol, count in fdist_punctuation.items():\n",
    "    print(f\"\\t{symbol} => {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dollar amount tokens:\n",
      "['$2,739', '$821', '$2,160', '$496', '$992', '$467', '$62.9\\xa0million', '$100,000', '$250,000', '$350,000', '$64\\xa0million', '$250,000', '$100', '$100', '$25', '$50', '$50', '$12', '$14', '$15', '$16', '$3.7\\xa0billion', '$40', '$65', '$40', '$481', '$481', '$0', '$84', '$12', '$92', '$27', '$84', '$0', '$24', '$0', '$24', '$20', '$12.6\\xa0million', '$4\\xa0million', '$500,000', '$1.50', '$499', '$329', '$170', '$170', '$100', '$10', '$50', '$10', '$10', '$0', '$4', '$0', '$4', '$0', '$0', '$0', '$1\\xa0million', '$4.1\\xa0million', '$60\\xa0million', '$2\\xa0billion', '$14', '$100']\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "'''\n",
    "Again, use nltk.regexp_tokenize() to tokenize the string you created in Question 2. This time, use a regexp that will tokenize dollar amounts. More specifically:\n",
    "\n",
    "Dollar amounts will begin with a '$' character.\n",
    "The next 1 to 3 characters will be digits.\n",
    "For larger numbers, the initial digits may be followed by groups consisting of a comma followed by three digits.\n",
    "Finally, there may be a single decimal point followed by an unlimited number of digits.\n",
    "The entire 'numerical' portion of the dollar amount might be followed by the word 'million' or 'billion' (with the first character lower- or upper-case).\n",
    "\n",
    "\n",
    "Some tips:\n",
    "\n",
    "Build your regexp incrementally, getting one portion correct before adding to it!\n",
    "To apply an operator (like * or +) to a sequence of characters, you may use brackets in the expected way. However, as we have discussed in class, brackets also indicate extraction groups. To use brackets to indicate the scope of operators without indicating extraction groups, the opening bracket must be followed by '?:'. E.g., \"(?:abc)* would indicate that the string 'abc' could appear 0 or more times.\n",
    "In your string version of the document, the spaces between the digits and the word million or billion is not a 'normal' space, but rather a unicode non-breaking space (with code \\xa0). It is suggested that you match these using a whitespace character (\\s).\n",
    "\n",
    "\n",
    "Print your entire list of tokens. (It shouldn't be a huge list!)\n",
    "'''\n",
    "\n",
    "import nltk # , requests\n",
    "# from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# url = \"https://budget.canada.ca/2024/report-rapport/chap3-en.html\"\n",
    "# response = requests.get(url)\n",
    "# html_content = response.text\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# text = soup.get_text(separator=' ', strip=True)\n",
    "# sentences = nltk.sent_tokenize(text)\n",
    "# cleaned_text = ' '.join(sentences)\n",
    "\n",
    "dollar_amount_regexp = r\"\"\"(?x)\n",
    "    \\$                # Starts with a dollar sign\n",
    "    \\d{1,3}           # Followed by 1 to 3 digits\n",
    "    (?:,\\d{3})*       # Optionally followed by comma and 3 digits (zero or more times)\n",
    "    (?:\\.\\d+)?        # Optionally followed by a decimal and one or more digits\n",
    "    (?:\\s[Mm]illion|\\s[Bb]illion)? # Optionally followed by 'million' or 'billion' (case-insensitive for first letter)\n",
    "\"\"\"\n",
    "\n",
    "dollar_tokens = regexp_tokenize(cleaned_text, dollar_amount_regexp)\n",
    "\n",
    "print(\"Dollar amount tokens:\")\n",
    "print(dollar_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
