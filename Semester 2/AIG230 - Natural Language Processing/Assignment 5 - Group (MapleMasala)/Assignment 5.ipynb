{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef72bd1",
   "metadata": {},
   "source": [
    "You have been provided with two sets of documents (in reality, each 'document' is a sentence). In one set, each sentence has been labelled as described below; in the second set, the sentences are unlabelled. In this workshop, you are to create a model (na√Øve Bayes) to classify the given sentences from the corpus into 5 categories: AIMX, OWNX, CONT, BASE, and MISC.\n",
    "\n",
    " \n",
    "\n",
    "Dataset:\n",
    "\n",
    "This corpus contains sentences from the abstract and introduction of scientific articles that have been annotated (labelled) using the following tags:\n",
    "\n",
    " \n",
    "\n",
    "AIMX: \"A specific research goal of the current paper\"\n",
    "\n",
    "OWNX: \"(Neutral) description of own work presented in current paper\"\n",
    "\n",
    "CONT: \"Statements of comparison with or contrast to other work; weaknesses of other work\"\n",
    "\n",
    "BASE: \"Statements of agreement with other work or continuation of other work\"\n",
    "\n",
    "MISC: \"(Neutral) description of other researchers' work\"\n",
    "\n",
    " \n",
    "\n",
    "For example, the following three sentences from the labelled dataset are tagged with MISC, AIMX, and OWNX.\n",
    "\n",
    " \n",
    "\n",
    "MISC The latter may also be applied to correct for an incidental large loop\n",
    "\n",
    "AIMX In this paper we apply the idea to graphical models for continuous variables\n",
    "\n",
    "OWNX We derive the loop corrected belief propagation equations for simple tractable Gaussian models, yielding a message passing scheme that, besides the correct average marginals, also yields the correct variances\n",
    "\n",
    " \n",
    "\n",
    "In addition to the labelled data, this corpus contains a set of unlabelled sentences. For example, the following two sentences are from the unlabelled dataset.\n",
    "\n",
    " \n",
    "\n",
    "here are also more than 1,100 additional genome sequencing projects currently underway around the world CITATION, CITATION.\n",
    "\n",
    "Convenient and effective computational methods are required to handle and analyze the immense amount of data generated by the whole-genome sequencing projects.\n",
    "\n",
    "\n",
    "\n",
    "Your task\n",
    "Preprocess the text to remove any stop words or punctuations (as necessary; see below).\n",
    "Vectorize the sentences (Choose TF-IDF or Word2Vec).\n",
    "Use Scikit-learn to create a Naive Bayes classifier. Train the classifier on the labelled dataset. Then, use the classifier to label the sentences in the unlabelled dataset.\n",
    "In addition to your code, submit the following:\n",
    "The accuracy of your classifier, comparing predictions made for the training set to the actual training set labels. (Divide your labeled dataset into two sets, training and testing.)\n",
    "A printout of each sentence in the unlabelled set, with each sentence preceded by the predicted class from your model.\n",
    "Summarize your work and your findings in a few sentences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Some important notes:\n",
    "As usual, you can make use of the code provided from this week's class, but it might not be as helpful this week. Some aspects of this week's code are not applicable at all (e.g., reading picked corpora). Moreover, other aspects are permissible, but not required for the assignment (e.g., pipelines, generating folds, applying k-fold cross-validation and measuring performance across folds). (If you want to take this approach, using pipelines and applying k-fold validation, that is fine, but it is not necessary.) You should definitely look at the code for building/applying models, though!\n",
    "Whether or not you tokenize/normalize your text before vectorizing depends on the vectorizer that you choose; look carefully at the usage of your chosen tool. There are many examples of tokenization code that you might use (e.g., that from the week 4 class). You will need to tweak it to meet the requirements for this assignment. (The normalization code from the Week 5 class depends on having text tagged with part-of-speech, so it won't be usable without modification.)\n",
    "(If you are not tokenizing, you will at least want to remove leaning/trailing whitespace from your sentences.)\n",
    "You will need to extract the labels from the labelled dataset. A simple way to do this would be to simply take the first token (if tokenized) or leading characters (if working with strings) from each sentence (ensuring it is a valid label, of course!)\n",
    "Given that you will be building scikit-learn models, it might make sense to use a scikit-learn vectorizer where applicable.\n",
    "If using a vector like the TfidfVectorizer, the tool needs the entire set (i.e., both the labelled and unlabelled sets) to build its vocabulary. You then, of course, will need separate sets of vectors for each of the labelled and unlabelled sets. The most direct way to do this is to call fit() using the combined sentences from both sets, and then call transform() on each set separately to generate the vectors for each set. (You might also try to use fit_and_transform, and then separate the results into to sets, but this raises more issues.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbf36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
